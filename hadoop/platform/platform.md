#### 大数据平台构成
- 存储计算平台
- 数据开发平台
- 数据采集系统
- 数据仓库
- 调度系统
- 数据服务系统


这几部分是相互依托的存在。

#### 存储计算平台
存储计算平台是基于CDH发行版，采用Hadoop生态HDFS、HBase、Hive、Kudu、spark、flink、presto等组件自建的大数据基础平台，监控利用CDH自带的CM监控管理模块。
  


#### 数据采集系统
数据采集系统主要完成异构数据源、多样化数据需求的的一站式数据采集，其依托于数据开发平台的配置管理、调度系统的任务调度等最终将数据汇集到存储和计算平台用于完成后续的数据加工和数据生产任务。基于Kafka为数据总线，按照数据源划分为MySQL数据源和非MySQL数据源，MySQL数据源采用canal监听MySQL的二进制日志推送到kafka，非MySQL数据源基于Kafka为数据总线，由业务方推送数据到数据总线。采用flink流计算框架开发实时数据采集模块将数据从数据总线中取出进行一定的清洗后分别存储到HDFS用于T+1的数据加工和生产，存储到Kudu用于实时数据需求。

#### 任务调度系统
基于airflow进行二次集成开发的任务调度平台。主要调度和管理HDFS到HIVE的数据清洗和加载任务、数据仓库层级间的数据加工任务，HIVE到MySQL的数据生产任务等。任务的调度执行可以在数据开发平台中进行

#### 数据服务系统
数据服务系统依托于任务调度系统中的数据生产任务生产的数据以及kudu提供的数据，以API形式对外提供数据服务接口调用，保证高可用性。对风控、资方和业务运营提供实时查询用户历史借款行为，用户APP点击事件行为查询服务，实时交易指标计算和展示等业务。

#### 数据仓库
数据仓库是一种逻辑模型，按照分层理论将数据主要物理存储在HIVE，根据在数据开发平台中配置的数据层级、策略、周期、表结构定义、SQL脚本，由自主开发的SparkETL框架自动完成数据加工任务的执行。

#### 数据开发平台。
数据开发平台设计的目标是简化数据接入的复杂度，降低数据接入门槛，提高数据效率，将大数据工程师从繁琐的取数逻辑中解放出来。主要有配置管理、任务定义和提交、数据监控、数据字典和权限管理等模块组成。数据产品经理，数据分析师、ETL工程师、数据开发工程师根据自己的权限配置管理和提交数据任务，任务提交后Commander服务发消息给kafka，Worker服务监听到kafka消息后从MySQL中读取任务配置异步完成对应的任务执行后更新任务状态。

任务监控和执行模块。主要完成数据仓库数据处理任务的

数据开发平台。主要简化数据开发的工作量，提升数据计入效率。主要给数据产品经理，数据分析师、ETL工程师、数据开发工程师提供配置管理接口、数据处理任务脚本提交接口、权限管理等功能。
任务调度平台。任务调度平台集成基于spark开发的任务框架，根据用户在数据开发平台上配置和提交的任务自动完成数据处理。

数据开发平台和任务执行模块之间通过kafka进行通信。

ETL任务按照数据处理流程划分为：
数据采集任务
数据加工任务
数据生产任务
按照时效性需求分为，实时和T+1，
T+1数据进入HIVE数据仓库，主要给数据分析师、数据产品提供数据支持。
实时数据进入KUDU，通过presto查询引擎提供给风控、实时指标大屏等应用提供数据服务。


主要由：
- 配置和管理模块。
  - 数据源管理
  - 元数据管理
  - 层级管理
  - 策略管理
  - 维度管理
- 任务提交模块
  - 表结构定义
  - SQL脚本提交
- 数据监控模块
  - 数据质量监控
  - 数据任务监控
- 数据字典